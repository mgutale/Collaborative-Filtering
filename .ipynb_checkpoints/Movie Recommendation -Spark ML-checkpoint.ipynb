{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems - Movie Recommendation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the Project\n",
    "\n",
    "In the beginning of 2006, Netflix, then a service peddling discs of every movie and TV show under the sun, announced “The Netflix Prize”, a competition that lured many engineers and programmers.  The mission was to make the company’s existing recommendation engine 10% more accurate.  The reward for solving this was one million dollars. Word of the competition immediately spread like a virus through computer science circles, technology bloggers, research communities, and even the mainstream media. And while a million dollars created attention, it was the data set -- over 100 million ratings of 17,770 movies from 480,189 customers -- that had number-crunching nuts salivating. There was nothing like it at the time and there hasn't been anything quite like it since. Following the announcement, 30,000 Netflix enthusiasts downloaded the information and set out to unlock this secret of the recommendation algorithm.  On June 26, 2009, a team called “BellKor’s Pragmatic Chaos” achieved a 10.5% improvement on Netflix’s recommending engine with Root Mean Squared Error known as RMSE of 0.8558. A month later another team “ Ensemble” achieved 10.09% RMSE over the Netflix’s and in September 2009, Netflix announced the BellKor’s team as the winner.  \n",
    "\n",
    "Recommendation algorithms are at the core of the Netflix products and many online retailers such as Amazon, Zara, Guardian, and many others. They provide customers with personalised suggestions to reduce the amount of time and frustration to find something great content to watch or buy. Because of the importance of the recommendations, many companies continually seek to improve them by advancing the state-of-the-art in the field. \n",
    "\n",
    "In this project I will be building a movie recommendation algorithm using the famous big dataset “MovieLens” dataset and Apache Spark utilities to handle the dataset and building the recommendation algorithm.  This is a fascinating project which scales well to big data and has many interesting applications and variety of ways to solve the problem and this is why I have chosen this project.  \n",
    "\n",
    "### Solution \n",
    "\n",
    "Assuming that rating matrix with mxn dimension where m is the user (500k) and n is the item (17k movies), this matrix would be extremely sparse with only 100 million ratings and remaining 8.4 billion ratings missing – about 99% of possible ratings – because users only rate a small portion of the movies.  The goal of the recommender algorithm is to predict those missing ratings so that user can be recommended of items of similar ratings or users items of similar to their tastes.  Given this very large matrix, the only feasible that competitors attempt this by performing dimensionality reduction which was the basis for the winning algorithm and can be done via matrix factorization.  Matrix factorization many advantages; when explicit feedback is not available such as ratings which most of the application don’t, an inference can be used using implicit feedback which indirectly reflects the user’s opinion by observing their behaviour including purchase history, browsing history, search patterns or even mouse movements.  \n",
    "\n",
    "\n",
    "\n",
    "### Types of Recommender Systems \n",
    "\n",
    "Two most ubiquitous types of recommender systems are Content-Based and Collaborative Filtering (CF). Collaborative filtering produces recommendations based on the knowledge of users’ attitude to items, that is it uses the “wisdom of the crowd” to recommend items. In contrast, content-based recommender systems focus on the attributes of the items and give you recommendations based on the similarity between them. In general, Collaborative filtering (CF) is the workhorse of recommender engines. The algorithm has a very interesting property of being able to do feature learning on its own, which means that it can start to learn for itself what features to use. CF can be divided into Memory-Based Collaborative Filtering and Model-Based Collaborative filtering. \n",
    "\n",
    "**Memory-Based Collaborative Filtering** - Memory-Based Collaborative Filtering approaches can be divided into two main sections: user-item filtering and item-item filtering. A user-item filtering will take a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. In contrast, item-item filtering will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations. A distance metric commonly used in recommender systems is cosine similarity, where the ratings are seen as vectors in n-dimensional space and the similarity is calculated based on the angle between these vectors. As these are memory based algorithms and easy to implement, they do not scale well to real world scenarios and does address the well known cold start problem.  \n",
    "\n",
    "**Model-based Collaborative Filtering** on another hand is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF. The goal of MF is to learn the latent preferences of users and the latent attributes of items from known ratings (learn features that describe the characteristics of ratings) to then predict the unknown ratings through the dot product of the latent features of users and items. When you have a very sparse matrix, with a lot of dimensions, by doing matrix factorization you can restructure the user-item matrix into low-rank structure, and you can represent the matrix by the multiplication of two low-rank matrices, where the rows contain the latent vector. You fit this matrix to approximate your original matrix, as closely as possible, by multiplying the low-rank matrices together, which fills in the entries missing in the original matrix.\n",
    "\n",
    "### SVD\n",
    "\n",
    "A well-known matrix factorization method is Singular value decomposition (SVD). Collaborative Filtering can be formulated by approximating a matrix X by using singular value decomposition. The winning team at the Netflix Prize competition used SVD matrix factorization models to produce product recommendations. Matrix X can be factorized to U, S and V. The U matrix represents the feature vectors corresponding to the users in the hidden feature space and the V matrix represents the feature vectors corresponding to the items in the hidden feature space.\n",
    "Dataset \n",
    "\n",
    "The MovieLens dataset were collected by GroupLens Research Project at the university of Minnesota.  Dataset consists of 100,000 ratings (1-5) from 943 users on 1682 movies where each user has rated at least 20 movies and simple demographic info for the users. The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up. I am using this dataset because its sufficiently large dataset to address the question of big data and applicable to this project which is building Movie Recommendation algorithm.  \n",
    "\n",
    "My hypothesis is that this dataset is normal under the circumstances and fits the distribution of similar datasets in the real world.  \n",
    "\n",
    "### Evaluating of the Model\n",
    "\n",
    "There are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is Root Mean Squared Error (RMSE). I will therefore be using this to evaluate the model between the ground truth of test set ratings column and predicted ratings.  \n",
    "\n",
    "### Technologies \n",
    "\n",
    "I will be using Hadoop file system to load the dataset from the <a href=\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\">grouplens.org</a> website using the command line interface once loaded I will then run a pyspark session on Jupyter-notebook to load the data from Hadoop. I will then be performing some cleaning of the data on Spark RDD/DataFrame following by some statistical analysis of the dataset then preparation for Spark ML for the final machine learning algorithm and prediction.   \n",
    "\n",
    "### Approach to the Project \n",
    "\n",
    "I will be taking the following steps to this project: \n",
    "\n",
    "1.\tImport dependencies and start a spark session\n",
    "2.  Load the dataset and merge with the user information\n",
    "3.\tPerform some explanatory analysis of the dataset \n",
    "4.\tPrepare the dataset for machine learning including train/test split and feature preparation \n",
    "5.\tTrain a collaborative filtering model using Alternating least squares (ALS)\n",
    "6.\tTest the model \n",
    "7.\tEvaluate the model using RMSE\n",
    "8.  Tune Hyper Parameters\n",
    "8.\tFinally recommend movies to users \n",
    "9.\tConclusions \n",
    "10. Referrences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import sum, col, desc, mean, count, round\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark Session\n",
    "app_name = 'Recommender Systems in Spark'\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(app_name)\\\n",
    "    .config('spark.some.config.option', 'some_value')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the main rating dataset as RDD\n",
    "lines = spark.read.text(\"ml-100k/u.data\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the movie id dataset as RDD\n",
    "lines1 = spark.read.text(\"ml-100k/u.item\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the cols by seperator \n",
    "parts = lines.map(lambda row: row.value.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the cols by seperator \n",
    "parts1 = lines1.map(lambda row: row.value.split(\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the first two columns of the RDD\n",
    "parts1 = parts1.map(lambda col: [col[i] for i in [0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns and change the dtypes to appropriete\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),rating=float(p[2]), timestamp=int(p[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns and change the dtypes to appropriete\n",
    "movieRDD = parts1.map(lambda p: Row(movieId=int(p[0]), movieName=str(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DF from the spark ratings RDD\n",
    "ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DF from the spark movie RDD\n",
    "movie = spark.createDataFrame(movieRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join ratings to movie dataframe\n",
    "ratings_table = ratings.alias('a').join(movie.alias('b'), ratings['movieId'] == movie['movieId'], 'left').select(\"a.userId\", \"a.movieId\", \"a.rating\", \"b.movieName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the total number of rows in the ratings dataset\n",
    "ratings_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId', 'movieId', 'rating', 'movieName']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the total number of columns \n",
    "ratings_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+\n",
      "|userId|movieId|rating|           movieName|\n",
      "+------+-------+------+--------------------+\n",
      "|   138|     26|   5.0|Brothers McMullen...|\n",
      "|   224|     26|   3.0|Brothers McMullen...|\n",
      "|    18|     26|   4.0|Brothers McMullen...|\n",
      "|   222|     26|   3.0|Brothers McMullen...|\n",
      "|    43|     26|   5.0|Brothers McMullen...|\n",
      "|   201|     26|   4.0|Brothers McMullen...|\n",
      "|   299|     26|   4.0|Brothers McMullen...|\n",
      "|    95|     26|   3.0|Brothers McMullen...|\n",
      "|    89|     26|   3.0|Brothers McMullen...|\n",
      "|   361|     26|   3.0|Brothers McMullen...|\n",
      "|   194|     26|   3.0|Brothers McMullen...|\n",
      "|   391|     26|   5.0|Brothers McMullen...|\n",
      "|   345|     26|   3.0|Brothers McMullen...|\n",
      "|   303|     26|   4.0|Brothers McMullen...|\n",
      "|   401|     26|   3.0|Brothers McMullen...|\n",
      "|   429|     26|   3.0|Brothers McMullen...|\n",
      "|   293|     26|   3.0|Brothers McMullen...|\n",
      "|   270|     26|   5.0|Brothers McMullen...|\n",
      "|   442|     26|   3.0|Brothers McMullen...|\n",
      "|   342|     26|   2.0|Brothers McMullen...|\n",
      "+------+-------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the top 20 lines \n",
    "ratings_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the number of unique users \n",
    "ratings_table.select('UserId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the number of unique movies \n",
    "ratings_table.select('MovieId').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------+\n",
      "|MovieId|RatingAverage|NumberofUsers|\n",
      "+-------+-------------+-------------+\n",
      "|    237|          4.0|          384|\n",
      "|    847|          4.0|           55|\n",
      "|    241|          4.0|          128|\n",
      "|    705|          4.0|          137|\n",
      "|    287|          4.0|           78|\n",
      "|    502|          4.0|           57|\n",
      "|    736|          4.0|           78|\n",
      "|    191|          4.0|          276|\n",
      "|    474|          4.0|          194|\n",
      "|     19|          4.0|           69|\n",
      "|    558|          4.0|           70|\n",
      "|     65|          4.0|          115|\n",
      "|    656|          4.0|           44|\n",
      "|    222|          4.0|          365|\n",
      "|    385|          4.0|          208|\n",
      "|    730|          4.0|           24|\n",
      "|    270|          4.0|          136|\n",
      "|    293|          4.0|          147|\n",
      "|    418|          4.0|          129|\n",
      "|    965|          4.0|           21|\n",
      "+-------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the average average ratings by movieID and Count of Raters where the number of Users is greater than 10\n",
    "ratings_table.groupBy(\"MovieId\").agg(round(mean(\"rating\"),0).alias(\"RatingAverage\"),count('UserId').alias(\"NumberofUsers\")).filter(col(\"NumberofUsers\") > 10).sort(desc(\"RatingAverage\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|           movieName|NumberofVotes|\n",
      "+--------------------+-------------+\n",
      "|    Star Wars (1977)|          583|\n",
      "|      Contact (1997)|          509|\n",
      "|        Fargo (1996)|          508|\n",
      "|Return of the Jed...|          507|\n",
      "|    Liar Liar (1997)|          485|\n",
      "|English Patient, ...|          481|\n",
      "|       Scream (1996)|          478|\n",
      "|    Toy Story (1995)|          452|\n",
      "|Air Force One (1997)|          431|\n",
      "|Independence Day ...|          429|\n",
      "|Raiders of the Lo...|          420|\n",
      "|Godfather, The (1...|          413|\n",
      "| Pulp Fiction (1994)|          394|\n",
      "|Twelve Monkeys (1...|          392|\n",
      "|Silence of the La...|          390|\n",
      "|Jerry Maguire (1996)|          384|\n",
      "|  Chasing Amy (1997)|          379|\n",
      "|    Rock, The (1996)|          378|\n",
      "|Empire Strikes Ba...|          367|\n",
      "|Star Trek: First ...|          365|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#movies with the highest ratings\n",
    "ratings_table.groupby('movieName').agg(count('userId').alias('NumberofVotes')).sort(desc(\"NumberofVotes\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|           movieName|AverageRatings|\n",
      "+--------------------+--------------+\n",
      "|Maya Lin: A Stron...|           5.0|\n",
      "|Pather Panchali (...|           5.0|\n",
      "|Entertaining Ange...|           5.0|\n",
      "|         Anna (1996)|           5.0|\n",
      "|     Star Kid (1997)|           5.0|\n",
      "|      Everest (1998)|           5.0|\n",
      "|Some Mother's Son...|           5.0|\n",
      "|They Made Me a Cr...|           5.0|\n",
      "|Santa with Muscle...|           5.0|\n",
      "|Aiqing wansui (1994)|           5.0|\n",
      "|Marlene Dietrich:...|           5.0|\n",
      "|Someone Else's Am...|           5.0|\n",
      "|Saint of Fort Was...|           5.0|\n",
      "|Great Day in Harl...|           5.0|\n",
      "|  Prefontaine (1997)|           5.0|\n",
      "|North by Northwes...|           4.0|\n",
      "|When We Were King...|           4.0|\n",
      "|Heavenly Creature...|           4.0|\n",
      "|    Notorious (1946)|           4.0|\n",
      "|       Psycho (1960)|           4.0|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_table.groupby('movieName').agg(round(mean('rating'),0).alias('AverageRatings')).sort(desc(\"AverageRatings\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|summary|            userId|          movieId|           rating|           movieName|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "|  count|            100000|           100000|           100000|              100000|\n",
      "|   mean|         462.48475|        425.53013|          3.52986|                null|\n",
      "| stddev|266.61442012750865|330.7983563255848|1.125673599144316|                null|\n",
      "|    min|                 1|                1|              1.0|'Til There Was Yo...|\n",
      "|    max|               943|             1682|              5.0|� k�ldum klaka (C...|\n",
      "+-------+------------------+-----------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show statistics \n",
    "ratings_table.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare for modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset for train/test split\n",
    "(training, test) = ratings_table.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train Model - ALS Collaborative Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train ALS model with initial params\n",
    "als = ALS(maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------------+----------+\n",
      "|userId|movieId|rating|           movieName|prediction|\n",
      "+------+-------+------+--------------------+----------+\n",
      "|   251|    148|   2.0|Ghost and the Dar...| 2.8130546|\n",
      "|    26|    148|   3.0|Ghost and the Dar...| 2.7410953|\n",
      "|   332|    148|   5.0|Ghost and the Dar...| 4.0903797|\n",
      "|   916|    148|   2.0|Ghost and the Dar...| 2.1030276|\n",
      "|   236|    148|   4.0|Ghost and the Dar...| 4.1956735|\n",
      "|   602|    148|   4.0|Ghost and the Dar...| 3.7098408|\n",
      "|   663|    148|   4.0|Ghost and the Dar...| 3.1607192|\n",
      "|   727|    148|   2.0|Ghost and the Dar...| 3.3720503|\n",
      "|   190|    148|   4.0|Ghost and the Dar...|  3.421185|\n",
      "|   363|    148|   3.0|Ghost and the Dar...| 2.7118642|\n",
      "|   308|    148|   3.0|Ghost and the Dar...|  2.647034|\n",
      "|   479|    148|   2.0|Ghost and the Dar...| 3.2181206|\n",
      "|   455|    148|   3.0|Ghost and the Dar...|   3.13608|\n",
      "|   891|    148|   5.0|Ghost and the Dar...| 3.1954665|\n",
      "|   552|    148|   3.0|Ghost and the Dar...| 3.0373685|\n",
      "|   870|    148|   2.0|Ghost and the Dar...| 2.2289264|\n",
      "|   733|    148|   3.0|Ghost and the Dar...| 1.2470127|\n",
      "|    59|    148|   3.0|Ghost and the Dar...| 2.9585698|\n",
      "|   717|    148|   3.0|Ghost and the Dar...| 3.7365098|\n",
      "|   757|    148|   4.0|Ghost and the Dar...| 3.0711873|\n",
      "+------+-------+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings predictions for the majority of the top 20 rows are pretty close however some are way off. We can improve on this by changing the parameters later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.1016716161905122\n"
     ]
    }
   ],
   "source": [
    "#evaluate using RMSE \n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Improve the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid search function to tune the best parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    grid search function to select the best model based on RMSE of\n",
    "    validation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    \n",
    "    validation_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
    "    \n",
    "    maxIter: int, max number of learning iterations\n",
    "    \n",
    "    regParams: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    ranks: list of float, one dimension of hyper-param tuning grid\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    The best fitted ALS model with lowest RMSE score on validation data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\").setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 latent factors and regularization = 0.1: validation RMSE is 0.9287849273255995\n",
      "15 latent factors and regularization = 0.5: validation RMSE is 1.0715229680513707\n",
      "15 latent factors and regularization = 0.9: validation RMSE is 1.3087345398118395\n",
      "20 latent factors and regularization = 0.1: validation RMSE is 0.9304680790220787\n",
      "20 latent factors and regularization = 0.5: validation RMSE is 1.0715109662450097\n",
      "20 latent factors and regularization = 0.9: validation RMSE is 1.3087348115726882\n",
      "25 latent factors and regularization = 0.1: validation RMSE is 0.9290379823458554\n",
      "25 latent factors and regularization = 0.5: validation RMSE is 1.0712263684125654\n",
      "25 latent factors and regularization = 0.9: validation RMSE is 1.3087365494830216\n",
      "\n",
      "The best model has 15 latent factors and regularization = 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_c940631cd6a1, rank=15"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_ALS(training, test, 10, [0.1,0.5,0.9], [15,20,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9287849273255995\n"
     ]
    }
   ],
   "source": [
    "# try the new hyper parameters\n",
    "als = ALS(maxIter= 10, regParam=0.1, rank = 15, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "#evaluate using RMSE \n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Recommend movies to Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the movies df to Pandas\n",
    "movies = movie.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_to_user(user_id, model, num_recommendations = 5):\n",
    "    \n",
    "    \"\"\"Recommend to user based on similar users and return the top movie names\"\"\"\n",
    "    \n",
    "    # Generate top 5 movie recommendations for each user\n",
    "    userRecs = model.recommendForAllUsers(num_recommendations)\n",
    "    \n",
    "    #filter for user 471\n",
    "    recommendations = userRecs.filter(userRecs.userId == user_id).toPandas()\n",
    "    recommendations = recommendations['recommendations']\n",
    "    \n",
    "    #extract movieId and score \n",
    "    rec_list = [v[0] for _ in recommendations for v in _]\n",
    "    rating_list = [v[1] for _ in recommendations for v in _]\n",
    "    \n",
    "    #Extract the movie title and match to movie Id\n",
    "    recommended_list = movies[movies.movieId.isin(rec_list)]\n",
    "    recommended_list.loc[:,'value'] = rating_list\n",
    "    recommended_list.sort_values('value',ascending = False)\n",
    "    \n",
    "    return recommended_list[['movieName']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rocket Man (1997)'],\n",
       " ['Turbulence (1997)'],\n",
       " ['In Love and War (1996)'],\n",
       " ['Rendezvous in Paris (Rendez-vous de Paris, Les) (1995)'],\n",
       " ['Love! Valour! Compassion! (1997)'],\n",
       " ['That Old Feeling (1997)'],\n",
       " ['Star Maps (1997)'],\n",
       " ['Tom and Huck (1995)'],\n",
       " ['Hurricane Streets (1998)'],\n",
       " ['Angel Baby (1995)']]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try user number 471\n",
    "recommend_to_user(471, model, num_recommendations = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Godfather, The (1972)'],\n",
       " ['Rear Window (1954)'],\n",
       " ['Mina Tannenbaum (1994)'],\n",
       " ['Boys, Les (1997)'],\n",
       " ['Angel Baby (1995)']]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also for user id number 251 and recommend top 5\n",
    "recommend_to_user(251, model, num_recommendations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this notebook, i have built a collaborative filtering recommender system with matrix factorization. Matrix factorization is great for solving many real world problem as i have touched on earlier.  The issues of recommender systems boil down to three main issues:\n",
    "\n",
    "1. Limiations and scalabity issue - with Matrix factorization; i have demonstrated that this solution can easily scale to large datasets and big data. \n",
    "2. Popularity bias - this refers to when the system recommends the movies with the most interactions without any personalisation and matrix. Because ALS model learns to factorize rating matrix into user and movie representation, it allows model to predict better personalised movie ratings for users. \n",
    "3. item cold-start problem - this is when when movies added to the catalogue have either none or very little interactions while recommender rely on the movie’s interactions to make recommendations although this is also one of its disadvantages as the matrix factorization technique may suffer from cold start problem in not being able to serve a recommendation for new users that have bought nothing. \n",
    "\n",
    "Because the ability of Matrix factorization to deal with all the above issues, it was the winner for the Netflix Prize competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Referrences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books <br>\n",
    "Minning of Massive Datasets (Third Addition) by Jure Leskovec & Anand Rajaraman and etal <br>\n",
    "Big Data Processing with Apache Spark by Manuel Ignacio Franco <br.\n",
    "Learning PySpark by Tomasz Drabas & Danny Lee <br>\n",
    "\n",
    "Web <br>\n",
    "<a href = 'https://en.wikipedia.org/wiki/Netflix_Prize'>Wikipedia </a> <br>\n",
    "<a href = 'https://www.thrillist.com/entertainment/nation/the-netflix-prize'> Netflix Blog </a> <br>\n",
    "<a href = 'https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-1-knn-item-based-collaborative-filtering-637969614ea'> TowardsDataScience </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
